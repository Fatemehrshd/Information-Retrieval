{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Advanced Boolean Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">This project aims to develop an Information Retrieval (IR) system that supports both standard Boolean queries and proximity queries. The system is designed to handle a collection of text documents, building an Inverted Index and a Positional Index to facilitate efficient document retrieval.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">For developing this system, the first step is reading document files which are prepared. But before that, we need to import some neccessary libraries:\n",
    "    <ul>\n",
    "        <li>\n",
    "            <b>NLTK</b>: Natural Language ToolKit: A leading platform for building Python programs to work with human language data\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>OS</b>: A built-in module that provides a wide range of functions for interacting with the operating system. It allows you to work with file systems, directories, paths, environment variables, and more.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>String</b>: A package which provides a wide range of string manipulation functions and methods\n",
    "        </li>\n",
    "    </ul>\n",
    "\n",
    "Also we import some specific parts of NLTK which are described later:\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Stopwords</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Regexp_tokenize</b>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current directory, we have a file named **data** which contains our text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of data files here\n",
    "folder_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `os.listdir()` we make a list of all files and directories in the `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all files in \"data\" directory which are our text files\n",
    "file_list = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As there might exist some files which are not text or some directories, we use the below code to extract `.txt` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document titles:\n",
      "----------------------------------------\n",
      "Jerry Decided To Buy a Gun.txt\n",
      "Rentals at the Oceanside Community.txt\n",
      "Gasoline Prices Hit Record High.txt\n",
      "Cloning Pets.txt\n",
      "Crazy Housing Prices.txt\n",
      "Man Injured at Fast Food Place.txt\n",
      "A Festival of Books.txt\n",
      "Food Fight Erupted in Prison.txt\n",
      "Better To Be Unlucky.txt\n",
      "Sara Went Shopping.txt\n",
      "Freeway Chase Ends at Newsstand.txt\n",
      "Trees Are a Threat.txt\n",
      "A Murder-Suicide.txt\n",
      "Happy and Unhappy Renters.txt\n",
      "Pulling Out Nine Tons of Trash.txt\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# text_files stores text files' names\n",
    "text_files = [file for file in file_list if file.endswith('.txt')]\n",
    "\n",
    "# print text files titles\n",
    "print(\"document titles:\\n\" + \"-\"*40)\n",
    "for text_file in text_files:\n",
    "    print(text_file)\n",
    "print( \"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"line-height: 2;\">\n",
    "As we mentioned above, some specific functions of NLTK package are needed.\n",
    "\n",
    "- **regexp_tokenize**: we use this function for tokenization, specifically for tokenizing text based on a regular expression pattern.\n",
    "- **stopwords**: This module specifically contains a list of common stop words for multiple languages, which in this project we set English.Some examples of English stopwords are: \"the\", \"and\", \"or\", \"is\", \"was\", ... .\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to download stopwords for preprocessing instructons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Fatemeh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# download stopwords here\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">\n",
    "In this step, we want to preprocess text files functionally. Below steps have to be implemented:\n",
    "    <ol>\n",
    "        <li>\n",
    "            <b>tokenization</b>: which we will do it using `regexp_tokenize()` function from `NLTK` library. For this job, we need to specify a pattern for natural language. As in our documents we have words, numbers with less or equal to 3 digits, numbers with more than 3 digits which are seperated 3 by 3, and floating point numbers, we have to use a pattern to handle all these types concurrently. We used `r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'` which is decoded below: <br>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    \\d{1,3}: This part matches one to three-digit numbers like \"4\", \"32\", \"879\", not \"1234\" or \"1,000\".\n",
    "                </li>\n",
    "                <li>\n",
    "                (?:,\\d{3})*: This part matches comma-separated thousands separators for numbers like \"75,000\".\n",
    "                </li>\n",
    "                <li>\n",
    "                (?:\\.\\d+)?: This part handles floating point numbers.\n",
    "                </li>\n",
    "                <li>\n",
    "                |: This symbol acts as an OR operator, allowing the regular expression to match either a number (as described above) or its next part.\n",
    "                </li>\n",
    "                <li>\n",
    "                \\w+: This part matches words, which consist of one or more word characters (letters, digits, and underscores). It's not limited to numbers and can match words like \"apple,\" \"word123,\" and \"my_variable.\"\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "      <li>\n",
    "          <b>Lowercase tokens</b>: as step of preprocessing, we have to convert all words' characters to lowercase. So, after query preprocessing, our job is easier.      \n",
    "      </li>\n",
    "        <li>\n",
    "         <b>Delete Stopwrds</b>: We have to delete stopwords(These are the words which are use frequently in documents and not neccessary in Information Retrival. There is a list of stopwords which is prepared for English and we downloaded above.). So, the process of retriving will be faster because there are less words for checking.\n",
    "        </li>\n",
    "        <li>\n",
    "           <b>Delete punctuations</b>: Trivially, punctuations are not needed when we want to retrive information. So, it is needed to omit all punctuations from all documents.\n",
    "        <\\li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    '''\n",
    "    Define a regular expression pattern for tokenization \n",
    "    (matching words, 3-digit numbers, numbers with more than 3 digits and \n",
    "    have thousands seperators in which n > 3, and floating-point numbers)\n",
    "    '''\n",
    "    pattern = r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
    "    \n",
    "    # Tokenize the text using the regular expression pattern    \n",
    "    tokens = regexp_tokenize(text, pattern)\n",
    "    \n",
    "    # Lowercase the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Get the set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Join tokens to return a string for each document\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trie tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define trie node and trie tree to make a trie tree as a permuterm index and inverted index.\n",
    "(Trie implementation is copied from ChatGPT with some addition because it is not asked in this course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self, char):\n",
    "        self.char = char\n",
    "        self.children = {}\n",
    "        self.is_end_of_term = False\n",
    "        self.postingList = {}\n",
    "        \n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode(\"\")\n",
    "        self.result = []\n",
    "    \n",
    "    # Function for inserting a new word in trie\n",
    "    def insert(self, term, docID, index):\n",
    "        node = self.root\n",
    "        for char in term:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode(char)\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_term = True\n",
    "        node.postingList[docID] = [index]            \n",
    "        \n",
    "    # Function for searching a word in trie\n",
    "    def search(self, term):\n",
    "        self.result = []\n",
    "        node = self.root\n",
    "        for char in term:\n",
    "            if char not in node.children:\n",
    "                return None\n",
    "            node = node.children[char]\n",
    "        self.dfs(node, term[:-1])\n",
    "        return self.result\n",
    "    \n",
    "    # This function uses dfs algorithm to find children of a node\n",
    "    def dfs(self, node, prefix):\n",
    "        if node.is_end_of_term:\n",
    "            self.result.append(prefix + node.char)\n",
    "        for child in node.children.values():\n",
    "            self.dfs(child, prefix + node.char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a trie tree\n",
    "trie_inverted_index = Trie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another function we have to import from `collections` module named `defaultdict` which creates dictionaries with default values for new intered keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">Now, after preprocessing, it is time to create our inverted index.<br>\n",
    "    An inverted index is a kind of data structure which is particularly a dictionary that stores distinct words of all documnet files as dictionary keys and the values of dictionay keys are the docIDs of documents which have the key word in theirselves. Also, for each docID, we store a list which contains the index of that specific word in the document which is the word appeared in.<br>\n",
    "    <span style=\"color:red;\">Note: </span>I used try-execpt because as I wanted to work with files and read them all using UTF-8 endoding, I confronted the error \"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 394: invalid start byte\". That means there exists a file that contains non-UTF-8 encoded characters. So, I used latin-1 encoding for these type of files.<br>\n",
    "    <span style=\"color:blue;\">Warnin: </span>code in line 53 through 55 should be only run once because in the fisrt time, the processed files are created and the content is added. If we run it more than one time, the content of each, will be duplicate which is a trash!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also there is a function ```generate_permuterm``` which creates all permuterms of a term and returns the list of permuterms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permuterm(term, docID, index):\n",
    "    # Add $ at the end of each term to determine this is the end of a word\n",
    "    term = term + \"$\"\n",
    "    permuterms = [term]\n",
    "    # Create all permuterms\n",
    "    for char in term:\n",
    "        term = term[-1] + term[0:-1]\n",
    "        permuterms.append(term)\n",
    "    for permuterm in permuterms:\n",
    "        trie_inverted_index.insert(permuterm, docID, index)\n",
    "    return permuterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inverted index using defaultdict() for simplicity\n",
    "inverted_index = defaultdict(dict)\n",
    "\n",
    "#Create permuterm index which has all words' permuterms as keys and all base words as values\n",
    "permuterm_index = defaultdict(dict)\n",
    "\n",
    "# Create an empty list which will contain docIDs\n",
    "docIDs = []\n",
    "\n",
    "# Define a variable for allocating docIDs to the files\n",
    "id = 1\n",
    "\n",
    "# Start to read document files\n",
    "for text_file in text_files:\n",
    "    \n",
    "    # Define docID to for the current file we want to read\n",
    "    file_id = id \n",
    "    \n",
    "    # Append the docID to the corresponding list\n",
    "    docIDs.append(id)\n",
    "    \n",
    "    # Join file_path and file name to make a full file path\n",
    "    file_path = os.path.join(folder_path, text_file)\n",
    "    \n",
    "    # Open files, read them, and store their content in file_content variable\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "    # Preprocess each file content using preprocessing \n",
    "    # function whivch is defined above\n",
    "    processed_text = preprocessing(file_content)\n",
    "    \n",
    "    # Create inverted index: check if each word exists in processed document or not\n",
    "    for index, token in enumerate(processed_text.split()):\n",
    "\n",
    "        # Generate all token's permuterms and add them in trie_inverted_index\n",
    "        permuterms = generate_permuterm(token, file_id, index)\n",
    "        \n",
    "        # # Expand permuterm_index dict.\n",
    "        for permuterm in permuterms:\n",
    "            if permuterm not in permuterm_index:\n",
    "                permuterm_index[permuterm][token] = [index]\n",
    "            else:\n",
    "                permuterm_index[permuterm][token].append(index)\n",
    "        \n",
    "        # Expand inverted_index dict.\n",
    "        if token in inverted_index:\n",
    "            \n",
    "            # If file_id of the token is in the corresponding list of the \n",
    "            # token in dictionary, just add its index to the corresponding list\n",
    "            if file_id in inverted_index[token]:\n",
    "                inverted_index[token][file_id].append(index)\n",
    "         \n",
    "            # Else, add the file_id and the token's index in the file to the dictionary\n",
    "            else:\n",
    "                inverted_index[token][file_id] = [index]\n",
    "\n",
    "        # If token is not in the dictionary, add token, its file id,\n",
    "        # and its index in the corresponding file to the dictionary\n",
    "        else:\n",
    "            inverted_index[token][file_id] = [index]\n",
    "            \n",
    "    # Prepare id variable for next file\n",
    "    id = id + 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For correcting the spelling of a query, we use Levenshtein distance as a criterian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    \n",
    "    # Create a matrix to store the distances between prefixes of s1 and s2\n",
    "    dp = np.zeros((m + 1, n + 1), dtype=int)\n",
    "\n",
    "    # Initialize the first row and first column\n",
    "    for i in range(m + 1):\n",
    "        dp[i, 0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0, j] = j\n",
    "\n",
    "    # Fill in the matrix\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i, j] = min(\n",
    "                dp[i - 1, j] + 1,  # Deletion\n",
    "                dp[i, j - 1] + 1,  # Insertion\n",
    "                dp[i - 1, j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    return dp[m, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```spell_correction``` finds the nearest word in ```inverted_index``` to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(query):\n",
    "    # Initialize an empty list to store corrected words\n",
    "    corrected_query = []\n",
    "    \n",
    "    # Split the input query into individual terms\n",
    "    for term in query.split():\n",
    "        corrected_word = None  # Initialize the corrected word as None\n",
    "        min_distance = float('inf')  # Set the minimum distance to positive infinity\n",
    "        \n",
    "        # Iterate through the inverted index\n",
    "        for word in inverted_index:\n",
    "            # Calculate the Levenshtein distance between the query term and each word in the index\n",
    "            distance = levenshtein_distance(term, word)\n",
    "            \n",
    "            # If the calculated distance is smaller than the minimum distance found so far\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance  # Update the minimum distance\n",
    "                corrected_word = word  # Update the corrected word to the closest match\n",
    "        \n",
    "        corrected_query.append(corrected_word)  # Append the closest matched word to the corrected query\n",
    "    \n",
    "    # Join the corrected words into a new query string\n",
    "    new_query = \" \".join(corrected_query)\n",
    "    return new_query  # Return the new corrected query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean/Proximity Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2 sorted lists which will give a sorted array of docIDs\n",
    "def merge(docID1, docID2):\n",
    "    result = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    flag = 1\n",
    "    while i < len(docID1) and j < len(docID2):\n",
    "        if docID1[i] == docID2[j]:\n",
    "            result.append(docID1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif docID1[i] < docID2[j]:\n",
    "            result.append(docID1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(docID2[j])\n",
    "            j += 1\n",
    "    if i == len(docID1):\n",
    "        if j < len(docID2):\n",
    "            for k in range(j, len(docID2)):\n",
    "                result.append(docID2[k])\n",
    "    if j == len(docID2):\n",
    "        if i < len(docID1):\n",
    "            for k in range(i, len(docID1)):\n",
    "                result.append(docID1[k])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">\n",
    "In this part, I defined a while loop to get the queries from user until he/she inters \"0\" keyword to interrup the search engine.<br>\n",
    "    As it is guaranteed that the queries are which of the forms below, I did the needed process.\n",
    "    <ul>\n",
    "        <li>\n",
    "            x OR y\n",
    "        </li>\n",
    "        <li>\n",
    "            x AND y\n",
    "        </li>\n",
    "        <li>\n",
    "            NOT x\n",
    "        </li>\n",
    "    </ul>\n",
    "    For this job, first of all, I defined 2 lists to store docIDs of query tokens(I mean x and y in the list above, not OR, NOT, and ANd) which contain docIDs of the files which have the token. Another list I defined, is \"terms\". It contains tokens of the query after splitting. And the last list is \"result\" which contains docIDs which we have to print as a response to each query. <br>\n",
    "    If the query contains \"NOT\" in it, do the preprocessing fo the single-word after \"NOT\" and store it in the term1 variable. Then, append the values of the word in dictionary to docIDs_1. Then, for implementing \"NOT\" instruction, search in docIDs list for IDs which are not it docIDs_1 and print it as the result.<br>\n",
    "    If the query contains \"OR\", \"AND\", or \"NEAR/\", we have to words in our query which we have to do the neccessary procces for them. But the common part for all these 3 cases is that we have to seperate 2 words and store them in term1 and term2 variables. The, append docIDs corresponding to each term to its corresponding docIDs list(for term1, store in docIDs_1 and for term2 store in docIDs_2) to store docIDs which each word appead in the corresponding document. Now, if \"OR\" is in the query, clearly we have to do the logical or on docIDs_1 and docIDs_2 and output the result. But if there is \"AND\" or \"NEAR/\" in the query, we have to intersect the docIDs_1 and cocIDs_2. If there is \"AND\", we obtained the result and give it to output. <br>\n",
    "    But if there is \"NEAR/\", after intersecting, we have to do more. Tracing on intersected documents and find the indecies which the words appeared in. If the distance is less or equal to the number after / in \"NEAR/\", add it to the result. If not, do the process for next document.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_proximity_query(query):\n",
    "    docIDs_1 = []\n",
    "    docIDs_2 = []\n",
    "    terms = []\n",
    "    result = []\n",
    "    terms = query.split()\n",
    "    \n",
    "    if \"NOT\" in query:\n",
    "        term1 = preprocessing(terms[1])\n",
    "        term1 = spell_correction(term1)\n",
    "        for key in inverted_index[term1]:\n",
    "            docIDs_1.append(key)\n",
    "        result = [docID for docID in docIDs if docID not in docIDs_1]\n",
    "\n",
    "    else:\n",
    "        term1 = preprocessing(terms[0])\n",
    "        term2 = preprocessing(terms[2])\n",
    "        term1 = spell_correction(term1)\n",
    "        term2 = spell_correction(term2)\n",
    "\n",
    "        for key in inverted_index[term1]:\n",
    "            docIDs_1.append(key)\n",
    "        for key in inverted_index[term2]:\n",
    "            docIDs_2.append(key)\n",
    "\n",
    "        if \"OR\" in query:\n",
    "            result = merge(docIDs_1, docIDs_2)\n",
    "\n",
    "        else:\n",
    "            intersection = list(filter(lambda value: value in docIDs_2, docIDs_1))\n",
    "\n",
    "            if \"NEAR/\" in query:\n",
    "                # Calculate number of words between terms[0] and terms[2]\n",
    "                near = terms[1][5:]\n",
    "                near = int(near[0])\n",
    "\n",
    "                # Check if terms[0] and terms[2] are near atmost \"near\" words\n",
    "                for i in range(len(intersection)):\n",
    "                    for index1 in inverted_index[term1][intersection[i]]:\n",
    "                        for index2 in inverted_index[term2][intersection[i]]:\n",
    "                            diff = abs(index1 - index2) - 1\n",
    "                            if diff <= near and diff >= 0:\n",
    "                                result.append(intersection[i])\n",
    "            else:\n",
    "                result = intersection\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildcard query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I handled wildcard query type in which if the user chose this type, this functions start to run and find all words which have the structure of the input query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function normalizes the wildcard query input and puts it's last ```*``` at the end of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prefix(term):\n",
    "    while term[-1] != \"*\":\n",
    "        term = term[-1] + term[0:-1]\n",
    "    return term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if the wildcard has 1 or 2 stars. If there are 2 stars in the wildcard, it makes a new prefix which includes last prefix + $ + last suffix and then search for this prefix in ```trie_inverted_index```. And if there exists only 1 star, it searchs for the normal query which is found by ```find_prefix``` function and searchs in ```trie_inverted_index```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard_query(query):\n",
    "    \n",
    "    stars = query.count(\"*\")\n",
    "    children = []\n",
    "    if stars == 2:\n",
    "        if query[0] == '*' and query[-1] == '*':\n",
    "            query = query[1:] + \"$*\"\n",
    "            children = trie_inverted_index.search(query)\n",
    "        else:\n",
    "            result1 = []\n",
    "            result2= []\n",
    "            prefix, mid, suffix = query.split(\"*\")\n",
    "            query1 = suffix+\"$\"+prefix\n",
    "            children = trie_inverted_index.search(query1)\n",
    "\n",
    "        for term in children:\n",
    "            if term in permuterm_index:\n",
    "                result1.extend(permuterm_index[term])\n",
    "\n",
    "        result1 = list(set(result1))\n",
    "        \n",
    "\n",
    "        for term in result1:\n",
    "            if mid in term[len(prefix):len(term)-len(suffix)]:\n",
    "                result2.append(term)\n",
    "\n",
    "    elif stars == 1:\n",
    "        result1 = []\n",
    "        result2 = []\n",
    "        inds = []\n",
    "        query = query + \"$\"\n",
    "        normal_query = find_prefix(query)\n",
    "        children = trie_inverted_index.search(normal_query[0:-1])\n",
    "        for term in children:\n",
    "            if term in permuterm_index:\n",
    "                result1.extend(permuterm_index[term])\n",
    "        result2 = list(set(result1))\n",
    "    return result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Wildcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If user chooses to inter some boolean wildcard query, this part will run. It checks which kind of boolean query is intered and then try to find the type of its componants and search for them in right ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_wildcard_query(query):\n",
    "    docIDs_1 = []\n",
    "    docIDs_2 = []\n",
    "    wildcard_1 = None\n",
    "    wildcard_2 = None\n",
    "    result = []\n",
    "    terms = query.split()\n",
    "    \n",
    "    if \"NOT\" in query:\n",
    "        \n",
    "        term1 = terms[1]\n",
    "        wildcard_1 = wildcard_query(term1)\n",
    "        for word in wildcard_1:\n",
    "            for key in inverted_index[word]:\n",
    "                docIDs_1.append(key)\n",
    "        result = [docID for docID in docIDs if docID not in docIDs_1]\n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        term1 = terms[0]\n",
    "        term2 = terms[2]\n",
    "        \n",
    "        if \"*\" in term1:\n",
    "            wildcard_1 = wildcard_query(term1)\n",
    "            for word in wildcard_1:\n",
    "                for key in inverted_index[word]:\n",
    "                    docIDs_1.append(key)\n",
    "            term1 = wildcard_1    \n",
    "            if \"*\" in term2:\n",
    "                wildcard_2 = wildcard_query(term2)\n",
    "                for word in wildcard_2:\n",
    "                    for key in inverted_index[word]:\n",
    "                        docIDs_2.append(key)\n",
    "                term2 = wildcard_2\n",
    "                \n",
    "            elif \"*\" not in term2:\n",
    "                term2 = preprocessing(term2)\n",
    "                term2 = spell_correction(term2)\n",
    "                for key in inverted_index[term2]:\n",
    "                    docIDs_2.append(key)\n",
    "                wildcard_2 = [term2]\n",
    "                \n",
    "                \n",
    "                \n",
    "        elif \"*\" not in term1:\n",
    "            term1 = preprocessing(term1)\n",
    "            term1 = spell_correction(term1)\n",
    "            for key in inverted_index[term1]:\n",
    "                docIDs_1.append(key)\n",
    "            wildcard_1 = [term1]\n",
    "            if \"*\" in term2:\n",
    "                wildcard_2 = wildcard_query(term2)\n",
    "                for word in wildcard_2:\n",
    "                    for key in inverted_index[word]:\n",
    "                        docIDs_2.append(key)\n",
    "                \n",
    "            elif \"*\" not in term2:\n",
    "                term2 = preprocessing(term2)\n",
    "                term2 = spell_correction(term2)\n",
    "                for key in inverted_index[term2]:\n",
    "                    docIDs_2.append(key)\n",
    "                wildcard_2 = [term2]\n",
    "                \n",
    "        if \"OR\" in query:\n",
    "            result = merge(docIDs_1, docIDs_2)\n",
    "        \n",
    "        else:\n",
    "            intersection = list(filter(lambda value: value in docIDs_2, docIDs_1))\n",
    "#             print(intersection)\n",
    "            if \"NEAR/\" in query:\n",
    "                    # Calculate number of words between terms[0] and terms[2]\n",
    "                near = terms[1][5:]\n",
    "                near = int(near)\n",
    "\n",
    "#                 for i in range()\n",
    "\n",
    "                for i in range(len(intersection)):\n",
    "                    for word1 in wildcard_1:\n",
    "                        for word2 in wildcard_2:\n",
    "                            for index1 in inverted_index[word1][intersection[i]]:\n",
    "                                for index2 in inverted_index[word2][intersection[i]]:\n",
    "                                    diff = abs(index1 - index2) - 1\n",
    "                                    if diff <= near and diff >= 0:\n",
    "                                        result.append(intersection[i])\n",
    "            else:\n",
    "                result = intersection\n",
    "        \n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is the time of taking query and try to retrive what usesr wants!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, user is asked to choose on of the 4 possible query types. Then, for each type, the corresponding function will start to run and do the searchnig process.\n",
    "<ol>\n",
    "    <li>Boolean query: Queries like x AND y, x OR y, nad NOT x.</li>\n",
    "    <li>Proximity query: Queries like x NEAR/n y</li>\n",
    "    <li>Wildcard: Queries like a*b*c</li>\n",
    "    <li>Spell correction: Queries which have spelling mistakes</li>\n",
    "    <li>Boolean Wildcard: Queries like x AND a*b*c</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "1\n",
      "jerry AND 30\n",
      "Output:  [1]\n",
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "2\n",
      "n*b*y\n",
      "Output:  ['nearby', 'nobody']\n",
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "3\n",
      "festival founders\n",
      "Output:  festival founders\n",
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "4\n",
      "gas OR JerRy\n",
      "Output:  [1, 3]\n",
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "4\n",
      "gas NEAR/5 gasoli*\n",
      "[3]\n",
      "Output:  [3]\n",
      "Choose query type:\n",
      "1. Boolean/Proximity\n",
      "2. Wildcard\n",
      "3. Spell Correction\n",
      "4. Bolean Wildcard\n",
      "0. Quit\n",
      "0\n",
      "Quit\n"
     ]
    }
   ],
   "source": [
    "query = None\n",
    "choice = None\n",
    "while True:\n",
    "    print(\"Choose query type:\\n1. Boolean/Proximity\\n2. Wildcard\\n3. Spell Correction\\n4. Bolean Wildcard\\n0. Quit\")\n",
    "    choice = int(input())\n",
    "    result3 = []\n",
    "    if choice == 0:\n",
    "        print(\"Quit\")\n",
    "        break\n",
    "    else:\n",
    "        query = input()\n",
    "        if choice == 1:\n",
    "            result3 = boolean_proximity_query(query)\n",
    "        elif choice == 2:\n",
    "            result3 = wildcard_query(query)\n",
    "        elif choice == 3:\n",
    "            result3 = spell_correction(query)\n",
    "        else:\n",
    "            result3 = boolean_wildcard_query(query)\n",
    "\n",
    "        print(\"Output: \", result3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
