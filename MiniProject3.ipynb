{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project centers on constructing an Information Retrieval (IR) system using the Block-Sorted Based Indexing (BSBI) algorithm. The BSBI algorithm facilitates the creation of an inverted index from a collection of text documents, and the objective is to implement key steps, including document preprocessing, term indexing, and the merging of intermediate index blocks. Also, Gamma codeing is implemented which is a variable-length coding technique, will optimize the representation of positive integers, improving storage efficiency and speeding up information retrieval, particularly for datasets with skewed distributions where smaller values are more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">For developing this system, the first step is reading document files which are prepared. But before that, we need to import some neccessary libraries:\n",
    "    <ul>\n",
    "        <li>\n",
    "            <b>NLTK</b>: Natural Language ToolKit: A leading platform for building Python programs to work with human language data\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>OS</b>: A built-in module that provides a wide range of functions for interacting with the operating system. It allows you to work with file systems, directories, paths, environment variables, and more.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>String</b>: A package which provides a wide range of string manipulation functions and methods\n",
    "        </li>\n",
    "    </ul>\n",
    "\n",
    "Also we import some specific parts of NLTK which are described later:\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Stopwords</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Regexp_tokenize</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>shutil</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>PorterStemmer</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>json</b>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of data file here\n",
    "folder_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all files in \"data\" directory which are our text files\n",
    "file_list = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document titles:\n",
      "----------------------------------------\n",
      "Jerry Decided To Buy a Gun.txt\n",
      "Rentals at the Oceanside Community.txt\n",
      "Gasoline Prices Hit Record High.txt\n",
      "Cloning Pets.txt\n",
      "Crazy Housing Prices.txt\n",
      "Man Injured at Fast Food Place.txt\n",
      "A Festival of Books.txt\n",
      "Food Fight Erupted in Prison.txt\n",
      "Better To Be Unlucky.txt\n",
      "Sara Went Shopping.txt\n",
      "Freeway Chase Ends at Newsstand.txt\n",
      "Trees Are a Threat.txt\n",
      "A Murder-Suicide.txt\n",
      "Happy and Unhappy Renters.txt\n",
      "Pulling Out Nine Tons of Trash.txt\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# text_files stores text files' names\n",
    "text_files = [file for file in file_list if file.endswith('.txt')]\n",
    "\n",
    "# print text files titles\n",
    "print(\"document titles:\\n\" + \"-\"*40)\n",
    "for text_file in text_files:\n",
    "    print(text_file)\n",
    "print( \"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it this part I want to divide document files 3 by 3, I imported <b>json</b> package here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into 5 directories. inverted index of each block will be stored in its corresponding folder.\n",
    "import shutil\n",
    "\n",
    "cnt = 0\n",
    "index = 1\n",
    "blocks = []\n",
    "\n",
    "# Divid documents 3 by 3 and add them to their corresponding folder\n",
    "for text_file in text_files:\n",
    "\n",
    "    # Make a new directory named \"block%index\"\n",
    "    if cnt % (3) == 0:\n",
    "        os.mkdir(\"data/block%i\" % (index))\n",
    "        \n",
    "        # Append block number ot the list of blocks\n",
    "        blocks.append(index)\n",
    "\n",
    "    # Definde path of new directory which we want to move our data\n",
    "    new_path = os.path.join(folder_path, 'block%i' % index)\n",
    "\n",
    "    # Definde path of old file which we want to move our data in it\n",
    "    old_path = os.path.join(folder_path, text_file)\n",
    "\n",
    "    # Move documents from \"data\" directory to the \"block%index\"\n",
    "    shutil.move(old_path, new_path)\n",
    "\n",
    "    # Increment cnt\n",
    "    cnt += 1\n",
    "\n",
    "    # If 3 docs are moved to the block%index folder, increment index\n",
    "    if cnt % 3 == 0: index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height: 2;\">\n",
    "In this step, we want to preprocess text files functionally. Below steps have to be implemented:\n",
    "    <ol>\n",
    "        <li>\n",
    "            <b>tokenization</b>: which we will do it using `regexp_tokenize()` function from `NLTK` library. For this job, we need to specify a pattern for natural language. As in our documents we have words, numbers with less or equal to 3 digits, numbers with more than 3 digits which are seperated 3 by 3, and floating point numbers, we have to use a pattern to handle all these types concurrently. We used `r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'` which is decoded below: <br>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    \\d{1,3}: This part matches one to three-digit numbers like \"4\", \"32\", \"879\", not \"1234\" or \"1,000\".\n",
    "                </li>\n",
    "                <li>\n",
    "                (?:,\\d{3})*: This part matches comma-separated thousands separators for numbers like \"75,000\".\n",
    "                </li>\n",
    "                <li>\n",
    "                (?:\\.\\d+)?: This part handles floating point numbers.\n",
    "                </li>\n",
    "                <li>\n",
    "                |: This symbol acts as an OR operator, allowing the regular expression to match either a number (as described above) or its next part.\n",
    "                </li>\n",
    "                <li>\n",
    "                \\w+: This part matches words, which consist of one or more word characters (letters, digits, and underscores). It's not limited to numbers and can match words like \"apple,\" \"word123,\" and \"my_variable.\"\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "      <li>\n",
    "          <b>Lowercase tokens</b>: as step of preprocessing, we have to convert all words' characters to lowercase. So, after query preprocessing, our job is easier.      \n",
    "      </li>\n",
    "        <li>\n",
    "         <b>Delete Stopwrds</b>: We have to delete stopwords(These are the words which are use frequently in documents and not neccessary in Information Retrival. There is a list of stopwords which is prepared for English and we downloaded above.). So, the process of retriving will be faster because there are less words for checking.\n",
    "        </li>\n",
    "        <li>\n",
    "           <b>Delete punctuations</b>: Trivially, punctuations are not needed when we want to retrive information. So, it is needed to omit all punctuations from all documents.\n",
    "           </li>\n",
    "<li>\n",
    "           <b>Stemming</b>: The process of stemming a documnet will be done by stemming() function.\n",
    "           </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import <b>PorterStemmer</b> from <b>NLKT</b> to do the stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will do the stemming process for all words in a document and returns the new documnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(document):\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    for word in document.split():\n",
    "        word = ps.stem(word)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bemow fumction, we w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    '''\n",
    "    Define a regular expression pattern for tokenization \n",
    "    (matching words, 3-digit numbers, numbers with more than 3 digits and \n",
    "    have thousands seperators in which n > 3, and floating-point numbers)\n",
    "    '''\n",
    "    pattern = r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
    "\n",
    "    # Tokenize the text using the regular expression pattern    \n",
    "    tokens = regexp_tokenize(document, pattern)\n",
    "\n",
    "    # Lowercase the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Get the set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    # Stemming\n",
    "    document = stemming(' '.join(tokens))\n",
    "\n",
    "    # return the processed document\n",
    "    return document\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing the BSBI algorithm for inverted indexing, we make inverted indices for documnets in each block. In each inveeted index, we store terms as keys of dictionary and docIDs as postings. But the point is that these docIDs are not simple integers, they are gamma encoded and are some coded binary strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as I want to store blocks' inverted indicies in a json file as a dictionary, I imported json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will do the procedure of gamma encoding for a given integer docID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_encoder(docID):\n",
    "    docID_copy = docID\n",
    "\n",
    "    # set length of the unary and binary parts as 0\n",
    "    length = 0\n",
    "\n",
    "    # While docID > 1, we will divide it by 2 to gain its binary representation.\n",
    "    while docID > 1:\n",
    "        docID //= 2\n",
    "        length += 1\n",
    "        \n",
    "    # Make unery part with '0's and end them with s '1' as it is expected in gamma encoding\n",
    "    unery_part = '0' * length + '1'\n",
    "\n",
    "    # Find re reminder; actually find docID - 2^length\n",
    "    binary_part_num = docID_copy - 2 ** length\n",
    "\n",
    "    # Define an empty string to store binary part\n",
    "    binary_rep  = \"\"\n",
    "\n",
    "    # Make binary form of binary_part_num in a loop\n",
    "    while binary_part_num > 0:\n",
    "        r = binary_part_num % 2\n",
    "        binary_part_num //= 2\n",
    "\n",
    "        # If the reminder is 0, then we insert a 0 at the begining of the string\n",
    "        if r == 0:\n",
    "            binary_rep = \"0\" + binary_rep\n",
    "        \n",
    "        # IF the reminder is 1, then we insert a 1 at the begining of the string\n",
    "        else: binary_rep = \"1\" + binary_rep\n",
    "\n",
    "    # If binary part was not represented in \"length\" chars, insert some zeros at its begining.\n",
    "    if len(binary_rep) < length:\n",
    "        diff = length - len(binary_rep)\n",
    "        binary_rep = \"0\" * diff + binary_rep\n",
    "    \n",
    "    # Return the coded string\n",
    "    return unery_part + binary_rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will decode a gamma codded string  and return an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_decoder(gamma_code):\n",
    "\n",
    "    # Find power of 2 in the gamma code formula\n",
    "    length = 0\n",
    "\n",
    "    # find number of 0s at the begining of the coded string which is same as power of 2 in the gamma code formula\n",
    "    while length < len(gamma_code) and gamma_code[length] == '0':\n",
    "        length += 1\n",
    "    \n",
    "    # Split binary part of the string and convert it to its corresponding integer\n",
    "    binary_part = gamma_code[length+1:]\n",
    "    reminder = 0\n",
    "    for digit in binary_part:\n",
    "        if digit == '1':\n",
    "            reminder = reminder * 2 + 1\n",
    "        else: reminder = reminder * 2\n",
    "    \n",
    "    # Return decoded integer\n",
    "    return 2 ** length + reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list which will contain docIDs\n",
    "docIDs = []\n",
    "\n",
    "# Define a variable for allocating docIDs to the files\n",
    "id = 1\n",
    "\n",
    "# Define number of blocks which we want to divide our files into.\n",
    "docs_per_block = 3\n",
    "\n",
    "# Make inverted index for documents in each block.\n",
    "for block in blocks:\n",
    "\n",
    "    block_inverted_index = {}\n",
    "\n",
    "    # Go to the corresponding block\n",
    "    block_path = os.path.join(folder_path, \"block%i\" % block)\n",
    "    \n",
    "    # Make a list of block's files\n",
    "    block_list = os.listdir(block_path)\n",
    "    block_files = [file for file in block_list if file.endswith('.txt')]\n",
    "\n",
    "    # Open each document in each block, do the preprocessing procedure, and make its inverted index.\n",
    "    for text_file in block_files:\n",
    "\n",
    "        docIDs.append(id)\n",
    "\n",
    "        # Fild path of documnet file to open\n",
    "        file_path = os.path.join(block_path, text_file)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                file_content = file.read()\n",
    "            \n",
    "        # Documnet preprocessing \n",
    "        processed_text = preprocessing(file_content)\n",
    "\n",
    "        for term in processed_text.split():\n",
    "\n",
    "        # For each term in the document, code its postings values\n",
    "            gamma_encoded = gamma_encoder(id)\n",
    "\n",
    "            # If term is in the file's corresponding block inverted index, check if the docID is in postings or not\n",
    "            if term in block_inverted_index:\n",
    "                \n",
    "                # If docID is not in the postings, then append it to the postings\n",
    "                if gamma_encoded not in block_inverted_index[term]:\n",
    "                    block_inverted_index[term].append(gamma_encoded)\n",
    "                \n",
    "                # else, do nothing\n",
    "                else: continue\n",
    "\n",
    "            # If the docID is not in postings, then add it to the list\n",
    "            else:\n",
    "                block_inverted_index[term] = [gamma_encoded]\n",
    "        id += 1\n",
    "\n",
    "    # At the end, make a json file to store the block's iverted index in the disk\n",
    "    json_path = os.path.join(block_path, \"inverted_index%i.json\" % block)\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(block_inverted_index, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2 sorted lists which will give a sorted array of docIDs\n",
    "def merge(docID1, docID2):\n",
    "    decoded_docID1 = []\n",
    "    decoded_docID2 = []\n",
    "\n",
    "    # Decode all gamma codes in docID1\n",
    "    for id in docID1:\n",
    "        decoded_docID1.append(gamma_decoder(id))\n",
    "\n",
    "    # Decode all gamma codes in docID2\n",
    "    for id in docID2:\n",
    "        decoded_docID2.append(gamma_decoder(id))\n",
    "\n",
    "    docID1 = decoded_docID1\n",
    "    docID2 = decoded_docID2\n",
    "\n",
    "    # Start merging process\n",
    "    result = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(docID1) and j < len(docID2):\n",
    "        if docID1[i] == docID2[j]:\n",
    "            result.append(docID1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif docID1[i] < docID2[j]:\n",
    "            result.append(docID1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(docID2[j])\n",
    "            j += 1\n",
    "    if i == len(docID1):\n",
    "        if j < len(docID2):\n",
    "            for k in range(j, len(docID2)):\n",
    "                result.append(docID2[k])\n",
    "    if j == len(docID2):\n",
    "        if i < len(docID1):\n",
    "            for k in range(i, len(docID1)):\n",
    "                result.append(docID1[k])\n",
    "    \n",
    "    # return the merged list of encoded postings\n",
    "    encoded_result = []\n",
    "    for i in range(len(result)):\n",
    "        encoded_result.append(gamma_encoder(result[i]))\n",
    "    return encoded_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below block, we will merge blocks' inverted indices and store the merged dictionary on the disk. The point is that as blocks' inverted indices were created in the order of sorted docIDs, mering in this example is not neccessary. Because, lists does not have any intersection and each of them is sorted. Also, docIDs in block(i) inverted index are all smaller than docIDs in block(i+1) inverted index. But as it is desired to implement the algorithm completely, we merge them in O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, when I want to create the merged_inverted_index, I check if the current key in block_inverted_index is in the merged_inverted_index or not. If it is not there, then add the fisrt docID in block_inverted_index as the started docID, and for the rest of docIDs in block_inverted_index, compute their difference with the previous docID in the merged_inverted_index and then, append the difference to the merged_inverted_index. If the key is in the merged_inverted_index, I compute the docID of the last documnt which the key happened there(sum of the first docID and differences) and then, compute the difference between current docID and the last one in the merged_inverted_index. Finally, append the new difference to the merged_inverted_index.\n",
    "\n",
    "\n",
    "The point is that as we store differences, merging these values is not a good idea because then, the order of docIDs will disappear. So, as docIDs in block_inverted_indcies are in an increasing order, we prefer to append the new difference to the list instead of merge lists.\n",
    "\n",
    "But, if merging is required, there is a block of commented code in the below section which makes a merged_inverted_index dictionary in which the values for each key are as follows: the first docID which the key is appeared in, is the fisrt elemnt of the list and for other values, their difference with the first docID is stored in the list to do not change the order and merging 2 sorted lists will be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inverted_index = {}\n",
    "\n",
    "for block in blocks:\n",
    "    # Open the inverted index corresponding to each block\n",
    "    inverted_index_path = os.path.join(folder_path + \"/block%i\" % block, \"inverted_index%i.json\" % block)\n",
    "\n",
    "    with open(inverted_index_path, 'r') as json_file:\n",
    "        block_inverted_index = json.load(json_file)\n",
    "\n",
    "        # find keys of the block's inverted index dictionary\n",
    "        keys = []\n",
    "        for key in block_inverted_index.keys():\n",
    "            keys.append(key)\n",
    "\n",
    "        # For each key in keys, check wheather the key is in the merged list or not\n",
    "        for key in keys:\n",
    "\n",
    "            # If the key is in merged_inverted_index, just add its posting to its corresponding posting\n",
    "            if key in merged_inverted_index:\n",
    "                for id_ in block_inverted_index[key]:\n",
    "                    sum_ = 0\n",
    "                    for dID in merged_inverted_index[key]:\n",
    "                        sum_ += gamma_decoder(dID)\n",
    "                    difference = gamma_decoder(id_) - sum_\n",
    "                    merged_inverted_index[key] = merged_inverted_index[key] + [gamma_encoder(difference)]            \n",
    "            \n",
    "            # If the key is not in merged_inverted_index, append it and add its posting to its corresponding posting\n",
    "            else:\n",
    "                fisrt_docID = block_inverted_index[key][0]\n",
    "                merged_inverted_index[key] = [fisrt_docID]\n",
    "                for k in range(1, len(block_inverted_index[key])):\n",
    "                    difference = gamma_decoder(block_inverted_index[key][k]) - gamma_decoder(block_inverted_index[key][k-1])                   \n",
    "                    merged_inverted_index[key] = merged_inverted_index[key] + [gamma_encoder(difference)]            \n",
    "\n",
    "\n",
    "# If you want to store differences of docIDs with 1st one, except 1st one which is itsel in tne merged list, run below commented code:\n",
    "''' In fact, in each list of values corresponding to each key of the merged_inverted_index dictionary,\n",
    "the first value is the docID of the first documnet which the key happened and i-th value(i != 0) is \n",
    "the difference between i-th document which the key happened there and the docID of the 1st documnet \n",
    "which the key happened. '''\n",
    "# min_value = 0\n",
    "# for key in merged_inverted_index:\n",
    "#     for index in range(len(merged_inverted_index[key])):\n",
    "#         if index == 0:\n",
    "#             min_value = merged_inverted_index[key][index]\n",
    "#         else:\n",
    "#             merged_inverted_index[key][index] = gamma_encoder(gamma_decoder(merged_inverted_index[key][index]) - gamma_decoder(min_value))\n",
    "\n",
    "\n",
    "# store final inverted index in a seperate file on disk\n",
    "merged_inverted_index_path = os.path.join(folder_path, \"inverted_index.json\")\n",
    "with open(merged_inverted_index_path, 'w') as json_file:\n",
    "    json.dump(merged_inverted_index, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
